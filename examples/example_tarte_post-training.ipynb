{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21df600f",
   "metadata": {},
   "source": [
    "### Example: Three modes of TARTE post-training paradigms.\n",
    "\n",
    "In this example, we run TARTE on `movies` data with various post-training paradigms: <br>\n",
    "\n",
    "- Fine-tuning a specific task <br>\n",
    "- Table featurizer with frozen backbone <br>\n",
    "- Boosting a complementary model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93a7cf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the current working directory and import packages\n",
    "import os\n",
    "from pathlib import Path\n",
    "os.chdir(Path().cwd().parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b818be8",
   "metadata": {},
   "source": [
    "Let's first load the data and set the splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f2f9f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tarte_ai import load_data\n",
    "\n",
    "# Set basic specifications\n",
    "data_name = \"movies\"      # Name of the data\n",
    "num_train = 256     # Train-size\n",
    "random_state = 1    # Random_state\n",
    "\n",
    "data, configs = load_data(data_name)\n",
    "data.fillna(value=np.nan, inplace=True)\n",
    "\n",
    "target_name = configs[\"target_name\"]\n",
    "X = data.drop(columns=target_name)\n",
    "y = data[target_name]\n",
    "y = np.array(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = num_train, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feded7a",
   "metadata": {},
   "source": [
    "(1) To run finetuning, we simply use:\n",
    "\n",
    "- `TARTE_TablePreprocessor` to prepare the tables to a suitable input for the TARTE transformer.\n",
    "- `TARTEFinetuneRegressor` to run TARTE fine-tuning (similar to the `CARTERegressor` in CARTE package)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6110591f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model No. xx:  11%|█         | 56/500 [00:56<07:26,  1.01s/it]\n",
      "Model No. xx:  12%|█▏        | 58/500 [00:56<07:13,  1.02it/s]\n",
      "Model No. xx:  12%|█▏        | 59/500 [01:02<07:47,  1.06s/it]\n",
      "Model No. xx:  12%|█▏        | 60/500 [01:03<07:47,  1.06s/it]\n",
      "Model No. xx:  12%|█▏        | 61/500 [01:03<07:39,  1.05s/it]\n",
      "Model No. xx:  12%|█▏        | 59/500 [01:04<08:02,  1.09s/it]\n",
      "Model No. xx:  17%|█▋        | 83/500 [01:14<06:12,  1.12it/s]\n",
      "Model No. xx:  17%|█▋        | 84/500 [01:20<06:40,  1.04it/s]\n",
      "Model No. xx:  21%|██        | 104/500 [01:27<05:34,  1.19it/s]\n",
      "Model No. xx:  23%|██▎       | 115/500 [01:35<05:20,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The R2 score for TARTE-Ridge: 0.5811\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from tarte_ai import TARTE_TablePreprocessor, TARTEFinetuneRegressor\n",
    "\n",
    "preprocessor = TARTE_TablePreprocessor()\n",
    "X_train_tarte_ft = preprocessor.fit_transform(X_train, y_train)\n",
    "X_test_tarte_ft = preprocessor.transform(X_test)\n",
    "\n",
    "fixed_params = dict()\n",
    "fixed_params[\"num_model\"] = 10 # 10 models for the bagging strategy\n",
    "fixed_params[\"disable_pbar\"] = False # True if you want cleanness\n",
    "fixed_params[\"random_state\"] = 0\n",
    "fixed_params[\"device\"] = \"cpu\"\n",
    "fixed_params[\"num_heads\"] = 24\n",
    "fixed_params[\"n_jobs\"] = 10\n",
    "fixed_params[\"num_layers\"] = 1\n",
    "\n",
    "estimator = TARTEFinetuneRegressor(**fixed_params)\n",
    "estimator.fit(X=X_train_tarte_ft, y=y_train)\n",
    "\n",
    "y_pred = estimator.predict(X_test_tarte_ft)\n",
    "score = r2_score(y_test, y_pred)\n",
    "print(f\"\\nThe R2 score for TARTE-Ridge:\", \"{:.4f}\".format(score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8ce81f",
   "metadata": {},
   "source": [
    "(2) To run TARTE featurizer, we simply use:\n",
    "\n",
    "- `TARTE_TablePreprocessor` to prepare the tables to a suitable input for the TARTE transformer.\n",
    "- `TARTE_TableEncoder` to encode the tables to a suitable input for the TARTE transformer\n",
    "- A suiable estimator (RidgeCV in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74d9ff58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The R2 score for TARTE-Ridge: 0.5751\n"
     ]
    }
   ],
   "source": [
    "from tarte_ai import TARTE_TableEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "tarte_tab_prepper = TARTE_TablePreprocessor()\n",
    "tarte_tab_encoder = TARTE_TableEncoder(layer_index=2) # Can change which layer to extract the embeddings\n",
    "\n",
    "prep_pipe = Pipeline([(\"prep\", tarte_tab_prepper), (\"tabenc\", tarte_tab_encoder)])\n",
    "\n",
    "X_train_featurizer = prep_pipe.fit_transform(X_train, y_train)\n",
    "X_test_featurizer = prep_pipe.transform(X_test)\n",
    "\n",
    "estimator = RidgeCV()\n",
    "estimator.fit(X=X_train_featurizer, y=y_train)\n",
    "y_pred = estimator.predict(X_test_featurizer)\n",
    "score = r2_score(y_test, y_pred)\n",
    "print(f\"\\nThe R2 score for TARTE-Ridge:\", \"{:.4f}\".format(score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8b8939",
   "metadata": {},
   "source": [
    "(3) To run TARTE Boosting with complementary model, it requires an additional choice of the base model and bit of data input preparation. In this example, we use TabVec + TabPFNv2 as the base model. \n",
    "\n",
    "- `TableVectorizer` to prepare the input of the base model.\n",
    "- `TARTE_TablePreprocessor` to prepare the tables to a suitable input for the TARTE transformer.\n",
    "- `TARTE_TableEncoder` to encode the tables to a suitable input for the TARTE transformer.\n",
    "- Form a list of input suitable for `TARTEBoostRegressor_TabPFN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d43ddd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/parietal/store2/work/mkim/.local/miniconda3/envs/tarte_test/lib/python3.12/site-packages/sklearn/preprocessing/_encoders.py:246: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from skrub import TableVectorizer\n",
    "\n",
    "# Prepare tablevectorizer\n",
    "tabvec = TableVectorizer()\n",
    "X_train_tabvec = tabvec.fit_transform(X_train).to_numpy()\n",
    "X_test_tabvec = tabvec.transform(X_test).to_numpy()\n",
    "\n",
    "# Prepare input for TARTE-B Regressor\n",
    "# TARTE features are already prepared from TARTE-Featurizer above\n",
    "# It is very import to keep the order: TabVec and then TARTE\n",
    "X_train_boost = [(X_train_tabvec[i], X_train_featurizer[i]) for i in range(len(y_train))]\n",
    "X_test_boost = [(X_test_tabvec[i], X_test_featurizer[i]) for i in range(len(y_test))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ffd571",
   "metadata": {},
   "source": [
    "To run TabPFN it is advised to use gpus (thus, we set `device=\"cuda\"`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee4bb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The R2 score for TARTE-Ridge: 0.6133\n"
     ]
    }
   ],
   "source": [
    "from tarte_ai import TARTEBoostRegressor_TabPFN\n",
    "\n",
    "params_boost = dict()\n",
    "params_boost[\"device\"] = 'cuda'\n",
    "params_boost[\"model_names\"] = [\"tabpfn\", \"ridge\"]\n",
    "params_boost[\"fit_order\"] = \"fixed\"\n",
    "params_boost[\"device\"] = \"cuda\" # For TabPFN implmentation\n",
    "\n",
    "estimator = TARTEBoostRegressor_TabPFN(**params_boost)\n",
    "estimator.fit(X=X_train_boost, y=y_train)\n",
    "y_pred = estimator.predict(X_test_boost)\n",
    "score = r2_score(y_test, y_pred)\n",
    "print(f\"\\nThe R2 score for TARTE-Ridge:\", \"{:.4f}\".format(score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b42b248",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tarte_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
